import matplotlib.pyplot as plt
from keras.models import load_model
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from keras.utils import to_categorical
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns



num_rounds = 201

rounds = []
for i in range(num_rounds):
    rounds.append(str(i+1))

result = {'r2': [(0, 0.06518518179655075),
      	              (1, 0.4148148000240326),
      	              (2, 0.48148149251937866),
      	              (3, 0.5111111402511597),
      	              (4, 0.5051851868629456),
      	              (5, 0.47555556893348694),
      	              (6, 0.4962962865829468),
      	              (7, 0.4962962865829468),
      	              (8, 0.5051851868629456),
      	              (9, 0.5037037134170532),
      	              (10, 0.45629629492759705),
      	              (11, 0.46666666865348816),
      	              (12, 0.49925926327705383),
      	              (13, 0.4874074161052704),
      	              (14, 0.48444443941116333),
      	              (15, 0.4592592716217041),
      	              (16, 0.5081481337547302),
      	              (17, 0.4888888895511627),
      	              (18, 0.48148149251937866),
      	              (19, 0.4918518662452698),
      	              (20, 0.48444443941116333),
      	              (21, 0.4725925922393799),
      	              (22, 0.46222221851348877),
      	              (23, 0.4874074161052704),
      	              (24, 0.5081481337547302),
      	              (25, 0.517037034034729),
      	              (26, 0.49481481313705444),
      	              (27, 0.521481454372406),
      	              (28, 0.5259259343147278),
      	              (29, 0.5051851868629456),
      	              (30, 0.5303703546524048),
      	              (31, 0.5185185074806213),
      	              (32, 0.5185185074806213),
      	              (33, 0.512592613697052),
      	              (34, 0.5155555605888367),
      	              (35, 0.5051851868629456),
      	              (36, 0.5037037134170532),
      	              (37, 0.5140740871429443),
      	              (38, 0.4933333396911621),
      	              (39, 0.5274074077606201),
      	              (40, 0.517037034034729),
      	              (41, 0.49481481313705444),
      	              (42, 0.48444443941116333),
      	              (43, 0.5051851868629456),
      	              (44, 0.517037034034729),
      	              (45, 0.5037037134170532),
      	              (46, 0.48592591285705566),
      	              (47, 0.5051851868629456),
      	              (48, 0.5007407665252686),
      	              (49, 0.517037034034729),
      	              (50, 0.5081481337547302),
      	              (51, 0.5007407665252686),
      	              (52, 0.5051851868629456),
      	              (53, 0.5022222399711609),
      	              (54, 0.5022222399711609),
      	              (55, 0.5096296072006226),
      	              (56, 0.5051851868629456),
      	              (57, 0.5096296072006226),
      	              (58, 0.521481454372406),
      	              (59, 0.5155555605888367),
      	              (60, 0.5199999809265137),
      	              (61, 0.512592613697052),
      	              (62, 0.5199999809265137),
      	              (63, 0.5199999809265137),
      	              (64, 0.521481454372406),
      	              (65, 0.5037037134170532),
      	              (66, 0.5140740871429443),
      	              (67, 0.5199999809265137),
      	              (68, 0.5081481337547302),
      	              (69, 0.5140740871429443),
      	              (70, 0.4888888895511627),
      	              (71, 0.5303703546524048),
      	              (72, 0.5229629874229431),
      	              (73, 0.5303703546524048),
      	              (74, 0.5155555605888367),
      	              (75, 0.5081481337547302),
      	              (76, 0.5022222399711609),
      	              (77, 0.5022222399711609),
      	              (78, 0.5007407665252686),
      	              (79, 0.5155555605888367),
      	              (80, 0.517037034034729),
      	              (81, 0.5185185074806213),
      	              (82, 0.5140740871429443),
      	              (83, 0.512592613697052),
      	              (84, 0.5199999809265137),
      	              (85, 0.5096296072006226),
      	              (86, 0.5022222399711609),
      	              (87, 0.5111111402511597),
      	              (88, 0.5244444608688354),
      	              (89, 0.5244444608688354),
      	              (90, 0.5155555605888367),
      	              (91, 0.5199999809265137),
      	              (92, 0.5066666603088379),
      	              (93, 0.5185185074806213),
      	              (94, 0.5051851868629456),
      	              (95, 0.49925926327705383),
      	              (96, 0.4962962865829468),
      	              (97, 0.4977777898311615),
      	              (98, 0.5007407665252686),
      	              (99, 0.49925926327705383),
      	              (100, 0.4874074161052704),
      	              (101, 0.5140740871429443),
      	              (102, 0.5111111402511597),
      	              (103, 0.5229629874229431),
      	              (104, 0.5288888812065125),
      	              (105, 0.5140740871429443),
      	              (106, 0.5111111402511597),
      	              (107, 0.5244444608688354),
      	              (108, 0.5081481337547302),
      	              (109, 0.5111111402511597),
      	              (110, 0.48444443941116333),
      	              (111, 0.4977777898311615),
      	              (112, 0.4785185158252716),
      	              (113, 0.4962962865829468),
      	              (114, 0.4874074161052704),
      	              (115, 0.48592591285705566),
      	              (116, 0.4918518662452698),
      	              (117, 0.4888888895511627),
      	              (118, 0.5022222399711609),
      	              (119, 0.5155555605888367),
      	              (120, 0.49925926327705383),
      	              (121, 0.5199999809265137),
      	              (122, 0.512592613697052),
      	              (123, 0.5140740871429443),
      	              (124, 0.5051851868629456),
      	              (125, 0.5022222399711609),
      	              (126, 0.5007407665252686),
      	              (127, 0.5081481337547302),
      	              (128, 0.48592591285705566),
      	              (129, 0.5051851868629456),
      	              (130, 0.5259259343147278),
      	              (131, 0.5259259343147278),
      	              (132, 0.5303703546524048),
      	              (133, 0.521481454372406),
      	              (134, 0.5259259343147278),
      	              (135, 0.517037034034729),
      	              (136, 0.5303703546524048),
      	              (137, 0.5199999809265137),
      	              (138, 0.5066666603088379),
      	              (139, 0.5155555605888367),
      	              (140, 0.5244444608688354),
      	              (141, 0.517037034034729),
      	              (142, 0.5155555605888367),
      	              (143, 0.512592613697052),
      	              (144, 0.5185185074806213),
      	              (145, 0.5140740871429443),
      	              (146, 0.5155555605888367),
      	              (147, 0.512592613697052),
      	              (148, 0.5155555605888367),
      	              (149, 0.5081481337547302),
      	              (150, 0.5096296072006226),
      	              (151, 0.4977777898311615),
      	              (152, 0.49925926327705383),
      	              (153, 0.49925926327705383),
      	              (154, 0.4977777898311615),
      	              (155, 0.4918518662452698),
      	              (156, 0.4962962865829468),
      	              (157, 0.5051851868629456),
      	              (158, 0.49925926327705383),
      	              (159, 0.4888888895511627),
      	              (160, 0.4977777898311615),
      	              (161, 0.5022222399711609),
      	              (162, 0.512592613697052),
      	              (163, 0.5037037134170532),
      	              (164, 0.5066666603088379),
      	              (165, 0.5111111402511597),
      	              (166, 0.5022222399711609),
      	              (167, 0.49481481313705444),
      	              (168, 0.48592591285705566),
      	              (169, 0.5051851868629456),
      	              (170, 0.5229629874229431),
      	              (171, 0.5111111402511597),
      	              (172, 0.5199999809265137),
      	              (173, 0.5288888812065125),
      	              (174, 0.5185185074806213),
      	              (175, 0.5229629874229431),
      	              (176, 0.49481481313705444),
      	              (177, 0.4977777898311615),
      	              (178, 0.5051851868629456),
      	              (179, 0.5037037134170532),
      	              (180, 0.5081481337547302),
      	              (181, 0.5007407665252686),
      	              (182, 0.5140740871429443),
      	              (183, 0.5022222399711609),
      	              (184, 0.5185185074806213),
      	              (185, 0.517037034034729),
      	              (186, 0.5199999809265137),
      	              (187, 0.5096296072006226),
      	              (188, 0.5111111402511597),
      	              (189, 0.5199999809265137),
      	              (190, 0.5155555605888367),
      	              (191, 0.512592613697052),
      	              (192, 0.5037037134170532),
      	              (193, 0.5111111402511597),
      	              (194, 0.5274074077606201),
      	              (195, 0.517037034034729),
      	              (196, 0.5051851868629456),
      	              (197, 0.5199999809265137),
      	              (198, 0.49925926327705383),
      	              (199, 0.5066666603088379),
      	              (200, 0.5022222399711609)]}
      










global_r2 = []

global_mse = []

for i in range(len(result['r2'])):
      global_r2.append(result['r2'][i][1])
      

plt.figure(figsize=(20,6))
      
plt.plot(global_r2)

plt.xlabel('Round')

plt.ylabel('Accuracy')

plt.title('Accuracy of the global model')

plt.grid()

plt.show()    

model = load_model("/home/iman/projects/kara/Projects/T-Rize/federated_models_classification/FedProx/CNN/global_models/global_model/global_model_200.h5")

dataframes = []
for i in range(4):
	file_paths = f'/home/iman/projects/kara/Projects/T-Rize/archive/City_data_classification/subset_{i}.csv'
	df = pd.read_csv(file_paths)
	sampled_data = df.sample(frac=0.3)
	city = df['City'].iloc[0]
	dataframes.append(sampled_data)

dataset = pd.concat(dataframes, ignore_index=True)
unique_cities = dataset['City'].unique()
print()
print(f'central test dataset includes these cities: {unique_cities}')
print()
dataset = dataset.drop(columns=['Address', 'City', 'State', 'County', 'Zip Code', 'Latitude', 'Longitude', 'Price'])
dataset = dataset.dropna()
city_num = dataset['city_num'].unique()
print()
print('unique city num in the gobal test datset: ')
print(city_num)
print()

x_test = dataset.drop(columns=['price_label']).values
y_test = dataset['price_label']
y_test_categorical = to_categorical(y_test, num_classes=11)

class_names = []
for i in y_test.unique():
    class_names.append(str(i))
scaler = StandardScaler()
x_test = scaler.fit_transform(x_test)

y_pred = model.predict(x_test)

cm = confusion_matrix(y_test, np.argmax(y_pred, axis=1))
fig, ax = plt.subplots(figsize=(10, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=ax)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()
import matplotlib.pyplot as plt
import pickle
import pandas as pd
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from dataset import resplit, transform_dataset_to_dmatrix
from utils import BST_PARAMS
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import numpy as np



num_rounds = 200

rounds = []
for i in range(num_rounds):
    rounds.append(str(i+1))

result = {'r2': [(1, 0.45548961424332346),
      	         (2, 0.45548961424332346),
      	         (3, 0.45548961424332346),
      	         (4, 0.45548961424332346),
      	         (5, 0.45845697329376855),
      	         (6, 0.45845697329376855),
      	         (7, 0.456973293768546),
      	         (8, 0.46735905044510384),
      	         (9, 0.46439169139465875),
      	         (10, 0.46439169139465875),
      	         (11, 0.46735905044510384),
      	         (12, 0.4688427299703264),
      	         (13, 0.4688427299703264),
      	         (14, 0.47032640949554894),
      	         (15, 0.4732937685459941),
      	         (16, 0.4732937685459941),
      	         (17, 0.47032640949554894),
      	         (18, 0.47032640949554894),
      	         (19, 0.47181008902077154),
      	         (20, 0.4732937685459941),
      	         (21, 0.4732937685459941),
      	         (22, 0.47181008902077154),
      	         (23, 0.47774480712166173),
      	         (24, 0.486646884272997),
      	         (25, 0.486646884272997),
      	         (26, 0.486646884272997),
      	         (27, 0.4910979228486647),
      	         (28, 0.4910979228486647),
      	         (29, 0.4910979228486647),
      	         (30, 0.4910979228486647),
      	         (31, 0.4910979228486647),
      	         (32, 0.4970326409495549),
      	         (33, 0.5),
      	         (34, 0.5),
      	         (35, 0.5),
      	         (36, 0.4970326409495549),
      	         (37, 0.4970326409495549),
      	         (38, 0.4970326409495549),
      	         (39, 0.5014836795252225),
      	         (40, 0.5044510385756676),
      	         (41, 0.5014836795252225),
      	         (42, 0.5014836795252225),
      	         (43, 0.5089020771513353),
      	         (44, 0.5089020771513353),
      	         (45, 0.5089020771513353),
      	         (46, 0.5014836795252225),
      	         (47, 0.5014836795252225),
      	         (48, 0.5014836795252225),
      	         (49, 0.5089020771513353),
      	         (50, 0.5089020771513353),
      	         (51, 0.5014836795252225),
      	         (52, 0.5029673590504451),
      	         (53, 0.5074183976261127),
      	         (54, 0.5044510385756676),
      	         (55, 0.5089020771513353),
      	         (56, 0.5089020771513353),
      	         (57, 0.5089020771513353),
      	         (58, 0.516320474777448),
      	         (59, 0.516320474777448),
      	         (60, 0.516320474777448),
      	         (61, 0.516320474777448),
      	         (62, 0.5178041543026706),
      	         (63, 0.5178041543026706),
      	         (64, 0.5178041543026706),
      	         (65, 0.5178041543026706),
      	         (66, 0.5178041543026706),
      	         (67, 0.5178041543026706),
      	         (68, 0.5178041543026706),
      	         (69, 0.5178041543026706),
      	         (70, 0.5178041543026706),
      	         (71, 0.5178041543026706),
      	         (72, 0.5207715133531158),
      	         (73, 0.5207715133531158),
      	         (74, 0.5207715133531158),
      	         (75, 0.5222551928783383),
      	         (76, 0.5222551928783383),
      	         (77, 0.5222551928783383),
      	         (78, 0.5222551928783383),
      	         (79, 0.5222551928783383),
      	         (80, 0.5222551928783383),
      	         (81, 0.5237388724035609),
      	         (82, 0.5237388724035609),
      	         (83, 0.5237388724035609),
      	         (84, 0.5237388724035609),
      	         (85, 0.5237388724035609),
      	         (86, 0.5237388724035609),
      	         (87, 0.5237388724035609),
      	         (88, 0.5237388724035609),
      	         (89, 0.5237388724035609),
      	         (90, 0.5237388724035609),
      	         (91, 0.5237388724035609),
      	         (92, 0.5237388724035609),
      	         (93, 0.5237388724035609),
      	         (94, 0.5237388724035609),
      	         (95, 0.5237388724035609),
      	         (96, 0.5237388724035609),
      	         (97, 0.5237388724035609),
      	         (98, 0.5237388724035609),
      	         (99, 0.5237388724035609),
      	         (100, 0.5237388724035609),
      	         (101, 0.5237388724035609),
      	         (102, 0.5237388724035609),
      	         (103, 0.5237388724035609),
      	         (104, 0.526706231454006),
      	         (105, 0.526706231454006),
      	         (106, 0.526706231454006),
      	         (107, 0.526706231454006),
      	         (108, 0.526706231454006),
      	         (109, 0.526706231454006),
      	         (110, 0.526706231454006),
      	         (111, 0.526706231454006),
      	         (112, 0.526706231454006),
      	         (113, 0.526706231454006),
      	         (114, 0.5356083086053413),
      	         (115, 0.5356083086053413),
      	         (116, 0.5356083086053413),
      	         (117, 0.5356083086053413),
      	         (118, 0.5356083086053413),
      	         (119, 0.5356083086053413),
      	         (120, 0.5356083086053413),
      	         (121, 0.5356083086053413),
      	         (122, 0.5356083086053413),
      	         (123, 0.5356083086053413),
      	         (124, 0.5356083086053413),
      	         (125, 0.5356083086053413),
      	         (126, 0.5356083086053413),
      	         (127, 0.5356083086053413),
      	         (128, 0.5356083086053413),
      	         (129, 0.5356083086053413),
      	         (130, 0.5356083086053413),
      	         (131, 0.5356083086053413),
      	         (132, 0.5356083086053413),
      	         (133, 0.5356083086053413),
      	         (134, 0.5356083086053413),
      	         (135, 0.5356083086053413),
      	         (136, 0.5356083086053413),
      	         (137, 0.5356083086053413),
      	         (138, 0.5356083086053413),
      	         (139, 0.5370919881305638),
      	         (140, 0.5370919881305638),
      	         (141, 0.5370919881305638),
      	         (142, 0.5370919881305638),
      	         (143, 0.5370919881305638),
      	         (144, 0.5385756676557863),
      	         (145, 0.5385756676557863),
      	         (146, 0.5385756676557863),
      	         (147, 0.5385756676557863),
      	         (148, 0.5385756676557863),
      	         (149, 0.5385756676557863),
      	         (150, 0.5385756676557863),
      	         (151, 0.5385756676557863),
      	         (152, 0.5385756676557863),
      	         (153, 0.5385756676557863),
      	         (154, 0.5385756676557863),
      	         (155, 0.5385756676557863),
      	         (156, 0.5385756676557863),
      	         (157, 0.5385756676557863),
      	         (158, 0.5385756676557863),
      	         (159, 0.5385756676557863),
      	         (160, 0.5385756676557863),
      	         (161, 0.5385756676557863),
      	         (162, 0.5385756676557863),
      	         (163, 0.5385756676557863),
      	         (164, 0.5385756676557863),
      	         (165, 0.5385756676557863),
      	         (166, 0.5385756676557863),
      	         (167, 0.5385756676557863),
      	         (168, 0.5385756676557863),
      	         (169, 0.5385756676557863),
      	         (170, 0.5385756676557863),
      	         (171, 0.5385756676557863),
      	         (172, 0.5385756676557863),
      	         (173, 0.5385756676557863),
      	         (174, 0.5385756676557863),
      	         (175, 0.5385756676557863),
      	         (176, 0.5385756676557863),
      	         (177, 0.5385756676557863),
      	         (178, 0.5385756676557863),
      	         (179, 0.5385756676557863),
      	         (180, 0.5385756676557863),
      	         (181, 0.5385756676557863),
      	         (182, 0.5385756676557863),
      	         (183, 0.5385756676557863),
      	         (184, 0.5385756676557863),
      	         (185, 0.5385756676557863),
      	         (186, 0.5385756676557863),
      	         (187, 0.5385756676557863),
      	         (188, 0.5385756676557863),
      	         (189, 0.5385756676557863),
      	         (190, 0.5385756676557863),
      	         (191, 0.5385756676557863),
      	         (192, 0.5385756676557863),
      	         (193, 0.5385756676557863),
      	         (194, 0.5385756676557863),
      	         (195, 0.5385756676557863),
      	         (196, 0.5385756676557863),
      	         (197, 0.5385756676557863),
      	         (198, 0.5385756676557863),
      	         (199, 0.5385756676557863),
      	         (200, 0.5385756676557863)]}







global_r2 = []

global_mse = []

for i in range(len(result['r2'])):
      global_r2.append(result['r2'][i][1])
      

plt.figure(figsize=(20,6))
      
plt.plot(global_r2)

plt.xlabel('Round')

plt.ylabel('Accuracy')

plt.title('Accuracy of the global model')

plt.grid()

plt.show()    


with open('/home/iman/projects/kara/Projects/T-Rize/federated_models_classification/FedAvg/XGBoost/global_models/global_model_round_200.pkl', 'rb') as file:
    model = pickle.load(file)


dataframes = []
for i in range(4):
	file_paths = f'/home/iman/projects/kara/Projects/T-Rize/archive/City_data_classification/subset_{i}.csv'
	df = pd.read_csv(file_paths)
	sampled_data = df.sample(frac=0.3)
	city = df['City'].iloc[0]
	dataframes.append(sampled_data)

dataset = pd.concat(dataframes, ignore_index=True)
dataset = dataset.drop(columns=['Unnamed: 0', 'Address', 'City', 'State', 'County', 'Zip Code', 'Latitude', 'Longitude', 'Price'])
dataset = dataset.dropna()

test_set = dict()
test_set['inputs'] = dataset.drop(columns=['price_label'])
scaler = StandardScaler()
test_set['inputs'] = scaler.fit_transform(test_set['inputs'])

test_set['label'] = dataset['price_label']
# test_set.set_format("numpy")
test_dmatrix = transform_dataset_to_dmatrix(test_set)


y_true = test_dmatrix.get_label()
y_pred = model.predict(test_dmatrix)
print()

class_names = []
for i in np.unique(y_true):
    class_names.append(str(i))

cm = confusion_matrix(y_true, y_pred)
fig, ax = plt.subplots(figsize=(10, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=ax)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()
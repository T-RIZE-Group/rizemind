import matplotlib.pyplot as plt
from keras.models import load_model
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from keras.utils import to_categorical
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns



num_rounds = 201

rounds = []
for i in range(num_rounds):
    rounds.append(str(i+1))

result = {'r2': [(0, 0.08160237222909927),
      	              (1, 0.5637982487678528),
      	              (2, 0.6275964379310608),
      	              (3, 0.6305637955665588),
      	              (4, 0.6928783655166626),
      	              (5, 0.6557863354682922),
      	              (6, 0.637982189655304),
      	              (7, 0.6765578389167786),
      	              (8, 0.6810088753700256),
      	              (9, 0.6528189778327942),
      	              (10, 0.6617210507392883),
      	              (11, 0.6186943650245667),
      	              (12, 0.6127596497535706),
      	              (13, 0.6275964379310608),
      	              (14, 0.642433226108551),
      	              (15, 0.6216617226600647),
      	              (16, 0.6290801167488098),
      	              (17, 0.6186943650245667),
      	              (18, 0.6186943650245667),
      	              (19, 0.637982189655304),
      	              (20, 0.6350148320198059),
      	              (21, 0.6246290802955627),
      	              (22, 0.640949547290802),
      	              (23, 0.6216617226600647),
      	              (24, 0.6246290802955627),
      	              (25, 0.6216617226600647),
      	              (26, 0.6275964379310608),
      	              (27, 0.6157270073890686),
      	              (28, 0.6261127591133118),
      	              (29, 0.6246290802955627),
      	              (30, 0.6231454014778137),
      	              (31, 0.6320474743843079),
      	              (32, 0.6172106862068176),
      	              (33, 0.6513352990150452),
      	              (34, 0.6216617226600647),
      	              (35, 0.6320474743843079),
      	              (36, 0.6261127591133118),
      	              (37, 0.6172106862068176),
      	              (38, 0.6216617226600647),
      	              (39, 0.6246290802955627),
      	              (40, 0.6201780438423157),
      	              (41, 0.6172106862068176),
      	              (42, 0.6186943650245667),
      	              (43, 0.6320474743843079),
      	              (44, 0.6246290802955627),
      	              (45, 0.6275964379310608),
      	              (46, 0.6275964379310608),
      	              (47, 0.642433226108551),
      	              (48, 0.6305637955665588),
      	              (49, 0.6246290802955627),
      	              (50, 0.642433226108551),
      	              (51, 0.6246290802955627),
      	              (52, 0.6350148320198059),
      	              (53, 0.6231454014778137),
      	              (54, 0.6528189778327942),
      	              (55, 0.7329376935958862),
      	              (56, 0.6275964379310608),
      	              (57, 0.7255192995071411),
      	              (58, 0.642433226108551),
      	              (59, 0.6528189778327942),
      	              (60, 0.6439169049263),
      	              (61, 0.7091988325119019),
      	              (62, 0.6706231236457825),
      	              (63, 0.715133547782898),
      	              (64, 0.640949547290802),
      	              (65, 0.7284866571426392),
      	              (66, 0.6646884083747864),
      	              (67, 0.6617210507392883),
      	              (68, 0.6602373719215393),
      	              (69, 0.7062314748764038),
      	              (70, 0.6557863354682922),
      	              (71, 0.7002967596054077),
      	              (72, 0.637982189655304),
      	              (73, 0.7344213724136353),
      	              (74, 0.6498516201972961),
      	              (75, 0.7433234453201294),
      	              (76, 0.6528189778327942),
      	              (77, 0.7225519418716431),
      	              (78, 0.7047477960586548),
      	              (79, 0.6572700142860413),
      	              (80, 0.6810088753700256),
      	              (81, 0.7403560876846313),
      	              (82, 0.6557863354682922),
      	              (83, 0.7284866571426392),
      	              (84, 0.6795251965522766),
      	              (85, 0.721068263053894),
      	              (86, 0.6483679413795471),
      	              (87, 0.7403560876846313),
      	              (88, 0.6810088753700256),
      	              (89, 0.7462908029556274),
      	              (90, 0.6528189778327942),
      	              (91, 0.7136498689651489),
      	              (92, 0.781899094581604),
      	              (93, 0.7433234453201294),
      	              (94, 0.7329376935958862),
      	              (95, 0.7284866571426392),
      	              (96, 0.6646884083747864),
      	              (97, 0.7344213724136353),
      	              (98, 0.7017804384231567),
      	              (99, 0.7462908029556274),
      	              (100, 0.7537091970443726),
      	              (101, 0.7522255182266235),
      	              (102, 0.719584584236145),
      	              (103, 0.7551928758621216),
      	              (104, 0.6943620443344116),
      	              (105, 0.7626112699508667),
      	              (106, 0.7462908029556274),
      	              (107, 0.7596439123153687),
      	              (108, 0.7047477960586548),
      	              (109, 0.7492581605911255),
      	              (110, 0.7433234453201294),
      	              (111, 0.7047477960586548),
      	              (112, 0.6958457231521606),
      	              (113, 0.7596439123153687),
      	              (114, 0.6780415177345276),
      	              (115, 0.7448071241378784),
      	              (116, 0.6854599118232727),
      	              (117, 0.7418397665023804),
      	              (118, 0.7462908029556274),
      	              (119, 0.6884273290634155),
      	              (120, 0.7329376935958862),
      	              (121, 0.6988130807876587),
      	              (122, 0.7121661901473999),
      	              (123, 0.7522255182266235),
      	              (124, 0.6780415177345276),
      	              (125, 0.7462908029556274),
      	              (126, 0.7359050512313843),
      	              (127, 0.7418397665023804),
      	              (128, 0.7522255182266235),
      	              (129, 0.7448071241378784),
      	              (130, 0.6617210507392883),
      	              (131, 0.7626112699508667),
      	              (132, 0.6780415177345276),
      	              (133, 0.721068263053894),
      	              (134, 0.7700296640396118),
      	              (135, 0.715133547782898),
      	              (136, 0.7715133428573608),
      	              (137, 0.719584584236145),
      	              (138, 0.7477744817733765),
      	              (139, 0.7136498689651489),
      	              (140, 0.7448071241378784),
      	              (141, 0.7002967596054077),
      	              (142, 0.7388724088668823),
      	              (143, 0.7462908029556274),
      	              (144, 0.6988130807876587),
      	              (145, 0.7551928758621216),
      	              (146, 0.7002967596054077),
      	              (147, 0.7462908029556274),
      	              (148, 0.6661720871925354),
      	              (149, 0.7462908029556274),
      	              (150, 0.7418397665023804),
      	              (151, 0.7462908029556274),
      	              (152, 0.7077151536941528),
      	              (153, 0.7462908029556274),
      	              (154, 0.7002967596054077),
      	              (155, 0.7448071241378784),
      	              (156, 0.6988130807876587),
      	              (157, 0.7448071241378784),
      	              (158, 0.6617210507392883),
      	              (159, 0.7492581605911255),
      	              (160, 0.7566765546798706),
      	              (161, 0.721068263053894),
      	              (162, 0.7047477960586548),
      	              (163, 0.7418397665023804),
      	              (164, 0.6780415177345276),
      	              (165, 0.7626112699508667),
      	              (166, 0.7062314748764038),
      	              (167, 0.7581602334976196),
      	              (168, 0.6973294019699097),
      	              (169, 0.7611275911331177),
      	              (170, 0.7002967596054077),
      	              (171, 0.7359050512313843),
      	              (172, 0.6943620443344116),
      	              (173, 0.7551928758621216),
      	              (174, 0.6839762330055237),
      	              (175, 0.7566765546798706),
      	              (176, 0.7329376935958862),
      	              (177, 0.7611275911331177),
      	              (178, 0.7255192995071411),
      	              (179, 0.7492581605911255),
      	              (180, 0.7121661901473999),
      	              (181, 0.780415415763855),
      	              (182, 0.6988130807876587),
      	              (183, 0.7670623064041138),
      	              (184, 0.7136498689651489),
      	              (185, 0.7685459852218628),
      	              (186, 0.7774480581283569),
      	              (187, 0.783382773399353),
      	              (188, 0.7893174886703491),
      	              (189, 0.7759643793106079),
      	              (190, 0.7626112699508667),
      	              (191, 0.784866452217102),
      	              (192, 0.7462908029556274),
      	              (193, 0.7908011674880981),
      	              (194, 0.780415415763855),
      	              (195, 0.783382773399353),
      	              (196, 0.7477744817733765),
      	              (197, 0.780415415763855),
      	              (198, 0.7388724088668823),
      	              (199, 0.7551928758621216),
      	              (200, 0.7284866571426392)]}









global_r2 = []

global_mse = []

for i in range(len(result['r2'])):
      global_r2.append(result['r2'][i][1])
      

plt.figure(figsize=(20,6))
      
plt.plot(global_r2)

plt.xlabel('Round')

plt.ylabel('Accuracy')

plt.title('Accuracy of the global model')

plt.grid()

plt.show()    

model = load_model("/home/iman/projects/kara/Projects/T-Rize/federated_models_classification/FedAvg/MLP/global_models/global_model_200.h5")

dataframes = []
for i in range(4):
	file_paths = f'/home/iman/projects/kara/Projects/T-Rize/archive/City_data_classification/subset_{i}.csv'
	df = pd.read_csv(file_paths)
	sampled_data = df.sample(frac=0.3)
	city = df['City'].iloc[0]
	dataframes.append(sampled_data)

dataset = pd.concat(dataframes, ignore_index=True)
unique_cities = dataset['City'].unique()
print()
print(f'central test dataset includes these cities: {unique_cities}')
print()
dataset = dataset.drop(columns=['Address', 'City', 'State', 'County', 'Zip Code', 'Latitude', 'Longitude', 'Price'])
dataset = dataset.dropna()
city_num = dataset['city_num'].unique()
print()
print('unique city num in the gobal test datset: ')
print(city_num)
print()

x_test = dataset.drop(columns=['price_label']).values
y_test = dataset['price_label']
y_test_categorical = to_categorical(y_test, num_classes=11)

class_names = []
for i in y_test.unique():
    class_names.append(str(i))
scaler = StandardScaler()
x_test = scaler.fit_transform(x_test)

y_pred = model.predict(x_test)

cm = confusion_matrix(y_test, np.argmax(y_pred, axis=1))
fig, ax = plt.subplots(figsize=(10, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=ax)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()